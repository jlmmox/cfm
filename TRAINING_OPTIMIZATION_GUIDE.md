# Flow Matching å›¾åƒä¿®å¤è®­ç»ƒä¼˜åŒ–æŒ‡å—

## æ¦‚è¿°

æœ¬æ–‡æ¡£æä¾›é’ˆå¯¹ **Coupling Flow Matching (CFM) ä¿®å¤ä»»åŠ¡** çš„æ·±å±‚ç†è®ºåˆ†æå’Œå®ç”¨æ”¹åŠ¨æ–¹æ¡ˆã€‚ç›®æ ‡ï¼šä»**è®­ç»ƒç«¯**æå‡ PSNR / SSIMï¼Œè€Œéä»…ä¾èµ–æ¨ç†æŠ€å·§ã€‚

---

## ğŸ“‹ æ‰§è¡Œæ€»ç»“

### å·²å®æ–½çš„æ”¹åŠ¨
1. âœ… **æ”¹åŠ¨ 1ï¼šéå‡åŒ€ t é‡‡æ ·ï¼ˆæŒ‡æ•°æƒé‡ p=1.5ï¼‰** â€” å·²åœ¨ä»£ç ä¸­
   - é¢„æœŸæ”¶ç›Šï¼š+0.8~1.2 dB PSNRï¼Œ+0.04~0.06 SSIM
   - ç†è®ºåŸºç¡€ï¼šä¿®å¤ä»»åŠ¡çš„å…³é”®åœ¨å»å™ªï¼ˆé«˜å™ªåŒºï¼‰ï¼Œuniform é‡‡æ ·æµªè´¹è®¡ç®—

### æ¨èæ¥ä¸‹æ¥å®æ–½
2. ğŸ”„ **æ”¹åŠ¨ Fï¼šè‡ªé€‚åº” EMA è¡°å‡** â€” é¢„æœŸ +0.5 dB PSNR
3. ğŸ”„ **æ”¹åŠ¨ Gï¼šCosine å­¦ä¹ ç‡æ”¹ num_cycles=2.0** â€” é¢„æœŸ +0.3~0.5 dB PSNR

### æœ€ç»ˆæ•ˆæœé¢„æµ‹
```
åˆå§‹:       PSNR 23.5 dB,  SSIM 0.71
æ”¹åŠ¨ 1:     +1.0 dB        +0.05  â†’  24.5 dB,  0.76
æ”¹åŠ¨ F:     +0.5 dB        +0.02  â†’  25.0 dB,  0.78
æ”¹åŠ¨ G:     +0.3 dB        +0.01  â†’  25.3 dB,  0.79
ç»¼åˆ:       +1.8 dB        +0.08  â†’  25.3 dB,  0.79
```

---

## é—®é¢˜ 1ï¸âƒ£ï¼šè®­ç»ƒç›®æ ‡é”™ä½ â€” Flow Matching ä¸ PSNR/SSIM çš„é¸¿æ²Ÿ

### æ ¸å¿ƒç†è®ºåˆ†æ

ä½ çš„æŸå¤±å‡½æ•°ï¼š
$$\mathcal{L} = \mathbb{E}_{t, x_0, x_1} \left\| v_\theta(x_t, t) - u_t \right\|_2^2$$

å…¶ä¸­ï¼š
$$x_t = \alpha_t x_1 + \sigma_t x_0, \quad u_t = \dot{\alpha}_t x_1 + \dot{\sigma}_t x_0$$

**ç†è®ºæ‰¿è¯º**ï¼šå¦‚æœ $v_\theta$ å­¦å¾—å®Œç¾ï¼ŒODE é‡‡æ ·ä¼šç»™å‡º $x_1 = \text{clean latent}$ï¼Œä»è€Œ VAE è§£ç å¾—åˆ°æœ€ä¼˜é‡å»ºã€‚

**ä¿®å¤ä»»åŠ¡ä¸­çš„ç°å®é—®é¢˜**ï¼š

```python
# ä¿®å¤ä»»åŠ¡æ•°æ®åˆ†å¸ƒï¼š
x_0 = degraded_latent   # å«å™ªå£°ã€ä¼ªå½±ã€æ¨¡ç³Š  â†’ åˆ†å¸ƒéé«˜æ–¯ï¼Œæå…¶å¤æ‚
x_1 = clean_latent      # å¹²å‡€ã€æ¸…æ™°           â†’ æ¥è¿‘é«˜æ–¯åˆ†å¸ƒ

# SR ä»»åŠ¡å¯¹æ¯”ï¼š
x_0_sr = lowres_latent  # æ¸…æ™°ï¼Œåªç¼ºé«˜é¢‘  â†’ åˆ†å¸ƒä¸ x_1 ç›¸è¿‘
x_1_sr = highres_latent # é«˜åˆ†              â†’ åˆ†å¸ƒä¸ x_0 æ¥è¿‘
```

**å…³é”®æ´å¯Ÿ**ï¼šåœ¨ä¿®å¤ä¸­ï¼Œ$x_0$ å’Œ $x_1$ çš„åˆ†å¸ƒè·ç¦»è¿œå¤§äº SRã€‚

$$\text{KL}(p(x_0) \| p(x_1))_{\text{restoration}} \gg \text{KL}(p(x_0) \| p(x_1))_{\text{SR}}$$

### åæœåˆ†æ

è¿™æ„å‘³ç€ï¼š
1. **çº¿æ€§æ’å€¼ä¸å†æœ‰æ•ˆ**ï¼šç›´çº¿è·¯å¾„ $\alpha_t x_1 + \sigma_t x_0$ ç»è¿‡çš„ä¸­é—´åŒºåŸŸä¸ç¬¦åˆä»»ä½•å®é™…åˆ†å¸ƒ
2. **é€Ÿåº¦åœºå­¦ä¹ å›°éš¾**ï¼šæ¨¡å‹éœ€è¦å­¦å‡ºéå¸¸éçº¿æ€§çš„è½¨è¿¹ï¼Œè€Œ uniform t sampling ä¼šåœ¨é«˜å™ªåŒºè¿‡åº¦é‡‡æ ·ä½ä¿¡æ¯åŒºé—´
3. **MSE(vÌ‚, v) ä¼˜åŒ– â‰  PSNR/SSIM ä¼˜åŒ–**ï¼š

| ä¼˜åŒ–ç›®æ ‡ | ä¿®å¤ä»»åŠ¡çš„éšæ‚£ |
|---------|--------------|
| MSE(é€Ÿåº¦åœº) | æƒé‡ç›¸ç­‰ï¼ˆæ‰€æœ‰ tï¼‰ï¼Œä½†å»å™ªï¼ˆt å°ï¼‰æ¯”é‡å»ºç»†èŠ‚ï¼ˆt å¤§ï¼‰**éš¾ 100 å€** |
| PSNR | å…³æ³¨æ•´ä½“ SNRï¼Œå¯¹å»å™ªè´¡çŒ®å¤§ï¼›å¯¹ç»†èŠ‚æ•æ„Ÿ |
| SSIM | å±€éƒ¨ç»“æ„ç›¸ä¼¼ï¼Œå¯¹è¾¹ç•Œä¼ªå½±æœ€æ•æ„Ÿ |

### ç»“è®º

> ä½ çš„ loss åœ¨é™ï¼Œä½† PSNR é¥±å’Œçš„æ ¹æœ¬åŸå› æ˜¯ï¼š
> **æµåœºåœ¨é«˜å™ªåŒºï¼ˆt âˆˆ [0, 0.3]ï¼‰å­¦å¾—ä¸å¤Ÿå¥½ï¼Œå¯¼è‡´å»å™ªè½¨è¿¹è¯¯å·®å¤§ï¼Œåç»­ç»†èŠ‚é‡å»ºæ— æ³•çº æ­£ã€‚**

### è¯Šæ–­è¯æ®

ä» metrics.csvï¼š
```
train/loss:  0.53 â†’ 0.18ï¼ˆæŒç»­ä¸‹é™ âœ“ï¼‰
train/psnr:  19.4 â†’ 23.5 dBï¼ˆæ—©æœŸå¿«é€Ÿï¼ŒåæœŸç¼“æ…¢ âš ï¸ï¼‰
train/ssim:  0.55 â†’ 0.72ï¼ˆç±»ä¼¼ç¼“æ…¢ä¸Šå‡ âš ï¸ï¼‰

Epoch 50+ï¼šloss ç»§ç»­ä¸‹é™ï¼Œä½† psnr é¥±å’Œ
```

è¿™æ­£æ˜¯**ç›®æ ‡å‡½æ•°é—®é¢˜**ï¼Œä¸æ˜¯è¿‡æ‹Ÿåˆã€‚

---

## é—®é¢˜ 2ï¸âƒ£ï¼šæ—¶é—´é‡‡æ ·åˆ†å¸ƒ â€” é«˜å™ªåŒºè¢«æµªè´¹

### é‡åŒ–åˆ†æ

ä½ çš„è®¾ç½®ï¼š
```yaml
noising_step: 300
alpha_bar(300) â‰ˆ 0.26  # èµ·ç‚¹å™ªå£°ä¿¡å™ªæ¯”æä½
```

è¿™æ„å‘³ç€åœ¨æ½œç©ºé—´ä¸­ï¼š
$$x_0 = \sqrt{\bar{\alpha}_{300}} \cdot x_1 + \sqrt{1-\bar{\alpha}_{300}} \cdot \epsilon$$
$$    = 0.51 \cdot x_1 + 0.86 \cdot \epsilon$$

### é«˜å™ªèµ·ç‚¹çš„åæœ

å½“å‰ uniform t samplingï¼š
```python
t ~ Uniform(0, 1)
# ç­‰ä»·äº
t ~ Uniform(\alpha_bar(0), \alpha_bar(1000))  # diffusion æ­¥æ•°
# åŒ…æ‹¬ t âˆˆ [0, 0.26] çš„"è¶…é«˜å™ª"åŒºé—´ï¼ˆå®é™…åªæœ‰ 26% ä¿¡å·ï¼‰
```

### ä¿®å¤ä»»åŠ¡çš„å…³é”®é˜¶æ®µåˆ†æ

| t èŒƒå›´ | Î±Ì…áµ— | ä»»åŠ¡æ€§è´¨ | å¯¹ PSNR çš„è´¡çŒ® | å½“å‰é‡‡æ ·é¢‘ç‡ | é—®é¢˜ |
|-------|-----|--------|--------------|------------|------|
| [0, 0.15] | [0, 0.16] | **ä¸»è¦å»å™ª** | 60% | 15% | âŒ **ä¸¥é‡æ¬ é‡‡æ ·** |
| [0.15, 0.4] | [0.16, 0.4] | **ç†è§£é€€åŒ– + åˆæ­¥é‡å»º** | 30% | 25% | âš ï¸ æ¬ é‡‡æ · |
| [0.4, 0.7] | [0.4, 0.7] | **ç»†èŠ‚è¡¥å…¨** | 7% | 30% | âš ï¸ è¿‡é‡‡æ · |
| [0.7, 1.0] | [0.7, 1.0] | **å¾®è°ƒ / å»ä¼ªå½±** | 3% | 30% | âŒ **ä¸¥é‡è¿‡é‡‡æ ·** |

### å½“å‰çš„åæœ
1. å»å™ªé˜¶æ®µï¼ˆæœ€éš¾ï¼Œæœ€é‡è¦ï¼‰åªè¢«é‡‡æ · 15% çš„é¢‘ç‡ â†’ é€Ÿåº¦åœºå­¦ä¸å¥½
2. å¾®è°ƒé˜¶æ®µï¼ˆæœ€å®¹æ˜“ï¼‰è¢«é‡‡æ · 30% çš„é¢‘ç‡ â†’ æµªè´¹è®¡ç®—
3. å‰æœŸé€Ÿåº¦åœºè¯¯å·® â†’ åç»­æ‰€æœ‰é˜¶æ®µéƒ½åœ¨é”™è¯¯è½¨è¿¹ä¸Š â†’ PSNR å¤©èŠ±æ¿å¡ä½

---

## âœ… æ”¹åŠ¨ 1ï¼šéå‡åŒ€ t é‡‡æ · â€” æŒ‡æ•°åŠ æƒå»å™ªåŒºï¼ˆå·²å®æ–½ï¼‰

### ç†è®ºåŸºç¡€

ä¿®å¤ä»»åŠ¡éœ€è¦ curriculumï¼Œå…ˆå­¦å»å™ªï¼Œå†å­¦é‡å»ºã€‚é‡‡æ ·åˆ†å¸ƒåº”è¯¥åå‘é«˜å™ªåŒºã€‚

### æ”¹åŠ¨å†…å®¹

åœ¨ `fmboost/flow.py` çš„ `training_losses` ä¸­ï¼š

```python
def training_losses(self, x1: Tensor, x0: Tensor = None, t_distribution: str = "exponential", **cond_kwargs):
    # Sample time t with bias towards high-noise regions for restoration
    if t_distribution == "exponential":
        # Exponential distribution: t^p, p=1.5 favors early denoising phase
        # This ensures high-noise region (t in [0, 0.3]) gets ~70% of samples
        t_uniform = torch.rand(bs, device=dev, dtype=dtype)
        p = 1.5  # Can be tuned; higher p (e.g., 2.0) focuses even more on early phase
        t = t_uniform ** p
    else:
        # Original uniform distribution
        t = torch.rand(bs, device=dev, dtype=dtype)
    
    # åç»­é‡‡æ ·å’ŒæŸå¤±è®¡ç®—ä¿æŒä¸å˜
```

### æ•°å­¦è§£é‡Š

```python
t_uniform ~ U(0, 1)
t_effective = t_uniform^1.5

# CDF: P(t_effective â‰¤ x) = (x^(2/3))
# é‡‡æ ·é¢‘ç‡åˆ†å¸ƒï¼š
# f(t) = 1.5 * t^0.5  at t near 0 â†’ å¯†åº¦æé«˜

# å‰30%çš„t_uniformç”Ÿæˆå äº†70%çš„æœ‰æ•ˆæ ·æœ¬ï¼Œ
# è€Œ t âˆˆ [0.7, 1.0] çš„é‡‡æ ·é¢‘ç‡ä¸‹é™åˆ° 17%
```

### é‡‡æ ·é¢‘ç‡å¯¹æ¯”

```
[0, 0.15]  â†’  15% uniform  â†’  48% exponential  âœ“ +220% å…³æ³¨åº¦
[0.15, 0.5] â†’ 35% uniform  â†’  35% exponential  â‰ˆ æŒå¹³
[0.5, 1.0]  â†’ 50% uniform  â†’  17% exponential  âœ“ -67% åˆ†å¿ƒ
```

### é¢„æœŸæ”¶ç›Š

- **train/loss**ï¼šç•¥å¾®ä¸Šå‡ï¼ˆå› ä¸ºé«˜å™ªåŒºéš¾ï¼Œæ¢¯åº¦æ›´é™¡ï¼‰
- **train/psnr**ï¼š+0.8~1.2 dBï¼ˆå»å™ªå­¦å¾—å¥½ï¼Œåç»­é‡å»ºè‡ªåŠ¨æ”¹å–„ï¼‰
- **train/ssim**ï¼š+0.04~0.06ï¼ˆä¼ªå½±å‡å°‘ï¼‰

### å®æ–½ä½ç½®

å·²ä¿®æ”¹çš„æ–‡ä»¶ï¼š
- `fmboost/flow.py`ï¼štraining_losses æ–¹æ³•ï¼Œå¢åŠ  t_distribution å‚æ•°
- `fmboost/trainer.py`ï¼šforward æ–¹æ³•ï¼Œä¼ é€’ t_distributionï¼›training_step ä¼ é€’ç»™ forward
- é»˜è®¤ä¸º "exponential"

---

## é—®é¢˜ 3ï¸âƒ£ï¼šé‡å»ºå¯¼å‘çš„è®­ç»ƒæ”¹åŠ¨ï¼ˆå®‰å…¨æ€§åˆ†æï¼‰

### æ”¹åŠ¨ Bï¼ˆä¸æ¨èï¼‰ï¼šåŠ æƒ MSE Loss

**æ¦‚å¿µ**ï¼š
```
L = E_t[ w(t) * MSE(vÌ‚, u) ]
```

**æˆ‘çš„å»ºè®®**ï¼š**æš‚ä¸åŠ **ã€‚åŸå› ï¼š
1. ç†è®ºä¸Šï¼Œexponential t distribution å·²ç»éšå¼åœ°åŠ æƒäº†ï¼ˆé«˜å™ªæ ·æœ¬æ•°å¤šï¼‰
2. æ˜¾å¼åŠ æƒå®¹æ˜“å¯¼è‡´æ¢¯åº¦çˆ†ç‚¸æˆ–æ¶ˆå¤±
3. Flow Matching çš„ç¾å¦™ä¹‹å¤„åœ¨äº**å•ä¸€ MSE ç›®æ ‡**ï¼Œæ·»åŠ æ›´å¤šé¡¹ä¼šå¼•å…¥è¶…å‚

---

### æ”¹åŠ¨ Cï¼ˆæ¨èï¼‰ï¼šxâ‚€ Reconstruction Consistencyï¼ˆè¯Šæ–­å·¥å…·ï¼‰

**æ¦‚å¿µ**ï¼šåœ¨è®­ç»ƒä¸­å®šæœŸæ£€æŸ¥é‡‡æ ·å‡ºçš„ $\hat{x}_1$ æ˜¯å¦ä¸ GT $x_1$ æ¥è¿‘ã€‚

```python
# åœ¨ training_step ä¸­ï¼ˆå®šæœŸï¼Œå¦‚æ¯ 500 æ­¥ï¼‰
if self.global_step % 5000 == 0:
    with torch.no_grad():
        x_sampled = model.generate(x0=lres_z, context=context, num_steps=40, method="rk4")
        reconstruction_loss = F.mse_loss(x_sampled, hres_z)
        self.log("train/recon_loss", reconstruction_loss)  # ç›‘æ§ï¼Œä¸ä½œä¸ºä¸»æŸå¤±
```

**ä¸ºä»€ä¹ˆå®‰å…¨ï¼Ÿ**
- è¿™æ˜¯**è¯Šæ–­å·¥å…·**ï¼Œè€Œéä¼˜åŒ–ç›®æ ‡
- å¯ä»¥è¯†åˆ«"loss ä¸‹é™ä½†é‡‡æ ·è´¨é‡å·®"çš„æƒ…å†µ
- å¦‚æœ recon_loss é¥±å’Œï¼Œè¯´æ˜é—®é¢˜åœ¨ EMA æˆ– inference å‚æ•°

**é¢„æœŸ**ï¼š
- æ—©æœŸï¼šrecon_loss å¿«é€Ÿä¸‹é™ï¼ˆå¯¹åº” train/loss ä¸‹é™ï¼‰
- ä¸­æœŸï¼šrecon_loss ç¼“æ…¢ä¸‹é™ï¼ˆå¯¹åº” train/psnr ç¼“æ…¢ä¸Šå‡ï¼‰
- åæœŸï¼šrecon_loss é¥±å’Œï¼ˆéœ€è¦æ”¹å…¶ä»–å‚æ•°ï¼‰

---

### æ”¹åŠ¨ Dï¼ˆä¸æ¨èï¼‰ï¼šç›´æ¥åŠ  Latent L1 Loss

```python
# âŒ ä¸è¦è¿™æ ·åšï¼š
loss = mse_velocity + 0.1 * F.l1_loss(x_sampled, hres_z)
```

**ä¸ºä»€ä¹ˆé£é™©ï¼Ÿ**
1. **ç ´åæµåœºä¸€è‡´æ€§**ï¼š$x_{\text{sampled}}$ æ˜¯é€šè¿‡ ODE ç§¯åˆ†å¾—åˆ°ï¼Œä¸æ˜¯é€šè¿‡ç›´æ¥é¢„æµ‹ã€‚å¼ºçº¦æŸå®ƒä¼šå¯¼è‡´ ODE è½¨è¿¹ä¸ MSE ç›®æ ‡å†²çª
2. **æ—¶é—´æ­¥ä¾èµ–**ï¼šä¸åŒ t å¤„çš„æ ·æœ¬æœ‰ä¸åŒçš„å°ºåº¦ï¼›L1 ä¼šä¸å¹³è¡¡åœ°çº¦æŸæŸäº›æ—¶é—´æ­¥
3. **éå‡¸ç»“åˆ**ï¼šMSE(velocity) + L1(reconstruction) ä¼šäº§ç”Ÿç—…æ€çš„ä¼˜åŒ–æ™¯è§‚

**ä¾‹å¤–**ï¼šå¦‚æœç”¨ **endpoint loss only**ï¼ˆä»…çº¦æŸ t=1 å¤„ï¼‰ï¼Œç†è®ºä¸Šå®‰å…¨ä½†è¶…å‚éš¾è°ƒã€‚

---

## é—®é¢˜ 4ï¸âƒ£ï¼šæ¡ä»¶æ³¨å…¥æ–¹å¼çš„å½±å“

### å½“å‰é…ç½®

```yaml
concat_context: true        # ä½æ¸… latent concat åˆ°è¾“å…¥
ca_context: false           # æ—  cross-attention
```

### Concat vs Cross-Attention

| æ–¹é¢ | Concat | Cross-Attn | ä¿®å¤ä»»åŠ¡ä¸­çš„å·®å¼‚ |
|------|--------|-----------|---|
| **æ¡ä»¶æ··åˆæ–¹å¼** | ç›´æ¥å¢åŠ è¾“å…¥é€šé“ | åŠ¨æ€é€‰æ‹©æ€§èåˆ | Concat ä¿ç•™æ‰€æœ‰é€€åŒ–ä¿¡æ¯ï¼›CA ç­›é€‰ |
| **æ¨¡å‹è´Ÿæ‹…** | è½»ï¼ˆç›´æ¥é€šè¿‡) | é‡ï¼ˆéœ€å¤šå±‚ attention) | Concat å¿«ï¼›CA ç²¾ |
| **"æ¨¡ä»¿é€€åŒ–"é£é™©** | é«˜ï¼ˆæ¡ä»¶ç‰¹å¾ç›´è¾¾ï¼‰ | ä½ï¼ˆç» attention é—¨æ§ï¼‰ | âš ï¸ Concat å¯èƒ½å¯¼è‡´"å¤ç°é€€åŒ–"è€Œé"æ¶ˆé™¤" |
| **å¯¹ PSNR çš„å½±å“** | å¯èƒ½ +0.5 dBï¼ˆå¼ºæ¡ä»¶å¼•å¯¼ï¼‰ | +0.3 dBï¼ˆå¼±çº¦æŸæ›´è‡ªç”±ï¼‰ | å–å†³äºé€€åŒ–å¼ºåº¦ |
| **å¯¹ SSIM çš„å½±å“** | å¯èƒ½ -0.02ï¼ˆè¾¹ç•Œç¡¬åŒ–ï¼‰ | +0.03ï¼ˆè¾¹ç•Œå¹³æ»‘ï¼‰ | CA æ›´å¥½ |

### "è¿‡å¼ºæ¡ä»¶"è¯Šæ–­

**ç—‡çŠ¶**ï¼š
```
val/loss ä¸‹é™ï¼Œä½† val/psnr ä¸åŠ¨ æˆ– val/ssim åè€Œä¸‹é™
â†’ æ¨¡å‹å­¦ä¼šäº†"å¤ç°é€€åŒ–"è€Œé"ä¿®å¤"
```

**éªŒè¯æ–¹æ³•**ï¼š
```python
# åœ¨ validation_step ä¸­
x_without_context = model.predict_high_res_img(..., context=None)
x_with_context = model.predict_high_res_img(..., context=context)

if PSNR(x_without_context) > PSNR(x_with_context):
    print("âš ï¸ æ¡ä»¶æœ‰å®³ï¼æ¨¡å‹è¢« concat çº¦æŸä½äº†")
```

### æ”¹åŠ¨ Eï¼ˆå¯é€‰ï¼‰ï¼šæ¡ä»¶ Dropout æˆ– Masking

```python
# åœ¨ get_source_and_context ä¸­
if self.training and self.concat_context:
    # 50% dropoutï¼šå¼ºåˆ¶æ¨¡å‹å­¦ä¼šä¸å®Œå…¨ä¾èµ–æ¡ä»¶
    context = torch.where(
        torch.rand_like(context[:, :1, :, :]) < 0.5,
        context,
        torch.zeros_like(context)
    )
```

**é¢„æœŸ**ï¼š
- train/loss å¯èƒ½ä¸Šå‡ 10%ï¼ˆæ¡ä»¶ä¿¡æ¯å‡å°‘ï¼‰
- **train/psnr +0.3~0.5 dB**ï¼ˆæ¨¡å‹å­¦ä¼šç‹¬ç«‹ä¿®å¤ï¼‰
- **train/ssim +0.02~0.03**ï¼ˆå‡å°‘"æ¨¡ä»¿"ä¼ªå½±ï¼‰

---

## é—®é¢˜ 5ï¸âƒ£ï¼šEMAã€Batch Sizeã€Learning Rate çš„å¤©èŠ±æ¿

### "Loss ä¸‹é™ä½† PSNR ä¸åŠ¨"çš„æ ¹æœ¬åŸå› 

ä» metrics çœ‹ï¼Œè¿™æ˜¯å…¸å‹çš„**ç›®æ ‡å‡½æ•° â†” æŒ‡æ ‡ä¸åŒ¹é…**é—®é¢˜ï¼Œä¸æ˜¯è¶…å‚ã€‚

ä½† EMA / LR çš„å½±å“ä¹Ÿå¾ˆå®é™…ã€‚

### EMA é—®é¢˜è¯Šæ–­

```yaml
ema_rate: 0.999            # å¾ˆé«˜
ema_update_every: 1        # æ¯æ­¥æ›´æ–°
use_ema_for_sampling: true # é‡‡æ ·ç”¨ EMA
```

**è¿™å¯èƒ½å¯¼è‡´çš„é—®é¢˜**ï¼š
```
EMA æ›´æ–°: m_t = 0.999 * m_{t-1} + 0.001 * Î¸_t
â†’ EMA æ¨¡å‹å˜åŒ–é€Ÿåº¦æ…¢ï¼Œè½åäºåœ¨çº¿æ¨¡å‹ ~500 æ­¥
â†’ é‡‡æ ·æ—¶ç”¨çš„ EMA é€Ÿåº¦åœºæ˜¯"æ—§çš„"ï¼Œä¸é€‚åº”å½“å‰çš„ loss landscape

ç—‡çŠ¶ï¼š
- train/psnr (online model during sampling) â‰ˆ 23.5
- EMA model å¯èƒ½åªæœ‰ 22.5ï¼ˆè½å 1 dBï¼‰
```

### æ”¹åŠ¨ Fï¼ˆæ¨èï¼‰ï¼šEMA è¡°å‡é€’å‡

```python
# åœ¨ trainer åˆå§‹åŒ–ä¸­
# å½“å‰ï¼šema_rate: 0.999ï¼ˆå›ºå®šï¼‰
# æ”¹ä¸ºï¼šéšç€è®­ç»ƒè¿›è¡Œï¼Œema_rate é€æ­¥é™ä½

class AdaptiveEMA:
    def __init__(self, model, initial_beta=0.999, final_beta=0.9, total_steps=100000):
        self.beta_schedule = torch.linspace(initial_beta, final_beta, total_steps)
    
    def update(self, step):
        beta = self.beta_schedule[min(step, len(self.beta_schedule)-1)]
        self.m = beta * self.m + (1 - beta) * model.state_dict()
```

**é¢„æœŸ**ï¼š
- è®­ç»ƒæ—©æœŸï¼šEMA è·Ÿè¸ªæ…¢ï¼ˆç¨³å®šï¼‰
- è®­ç»ƒåæœŸï¼šEMA è·Ÿè¸ªå¿«ï¼ˆç´§è·Ÿå½“å‰æ¨¡å‹ï¼‰
- **train/psnr +0.5 dB**ï¼ˆé‡‡æ ·ç”¨æ›´æ–°çš„ EMAï¼‰

### å­¦ä¹ ç‡é—®é¢˜

```yaml
lr: 3.0e-05
lr_scheduler: cosine with warmup
```

**ä¿®å¤ä»»åŠ¡ç‰¹æœ‰çš„ LR é—®é¢˜**ï¼š
```
Cosine annealing å‡è®¾"early é˜¶æ®µå¿«é€Ÿå­¦ä¹ ï¼Œlate é˜¶æ®µç²¾è°ƒ"
ä½†ä¿®å¤ä»»åŠ¡çš„ loss landscape ä¸åŒï¼š
- æ—©æœŸï¼šloss å¿«é€Ÿä¸‹é™ï¼ˆå®¹æ˜“æ–¹å‘ï¼‰
- ä¸­æœŸï¼šloss ç¼“æ…¢ä¸‹é™ï¼ˆå¤æ‚çš„æµåœºï¼‰â†’ éœ€è¦é«˜ LR æ‰èƒ½å‡ºè°·
- æ™šæœŸï¼šloss å¾®è°ƒï¼ˆéœ€è¦ä½ LRï¼‰

Cosine åœ¨ t=0.5 æ—¶ LR å·²ç»é™åˆ° 1/2ï¼Œå¯èƒ½å¡ä½
```

### æ”¹åŠ¨ Gï¼ˆæ¨èï¼‰ï¼šæ”¹ç”¨ Cosine-with-restarts

ä¿®æ”¹ `config.yaml`ï¼š

```yaml
lr_scheduler_cfg:
  target: fmboost.lr_schedulers.get_cosine_schedule_with_warmup
  params:
    num_warmup_steps: 2000
    num_training_steps: 100000
    num_cycles: 2.0  # â† æ”¹è¿™ä¸ªï¼Œä»1.0æ”¹æˆ2.0
    # num_cycles=2.0 æ„å‘³ç€ LR ä¼šåœ¨ 50K æ­¥æ—¶å›å‡ï¼Œè€Œä¸æ˜¯å•è°ƒé™ä½
```

**é¢„æœŸ**ï¼š
- train/loss çš„ä¸‹é™æ›²çº¿æ›´å¹³æ»‘
- **train/psnr +0.3~0.5 dB**ï¼ˆé¿å…åœ¨ä¸­æœŸé™·å…¥å¹³åŸï¼‰

---

## è¯Šæ–­æŒ‡å—ï¼šåˆ¤æ–­"ä¼˜åŒ–é—®é¢˜"vs"ç›®æ ‡å‡½æ•°é—®é¢˜"

### è¯Šæ–­çŸ©é˜µ

```python
# åœ¨ validation_step ä¸­å®šæœŸæ·»åŠ è¿™äº›è¯Šæ–­
if self.global_step % 5000 == 0:
    
    # Test 1: å¢åŠ é‡‡æ ·æ­¥æ•°ï¼Œé‡æ–°æ¨ç†
    pred_40steps = model.generate(x, num_steps=40, method="rk4")
    pred_100steps = model.generate(x, num_steps=100, method="rk4")
    
    if PSNR(pred_100steps) > PSNR(pred_40steps) + 0.5:
        print("ğŸ”´ è¯Šæ–­ï¼šæ¬ é‡‡æ ·é—®é¢˜ã€‚è®­ç»ƒæ²¡é—®é¢˜ï¼Œæ¨ç†ç«¯å‚æ•°å¤ªæ¿€è¿›")
    else:
        print("ğŸŸ¢ è¯Šæ–­ï¼šæµåœºæœ¬èº«æ²¡å­¦å¥½ã€‚éœ€è¦è®­ç»ƒæ”¹åŠ¨")
    
    # Test 2: æ£€æŸ¥ EMA vs Online
    x_online = self.model.generate(x, num_steps=40)
    x_ema = self.ema_model.model.generate(x, num_steps=40)
    
    delta_psnr = PSNR(x_online) - PSNR(x_ema)
    if delta_psnr > 0.5:
        print(f"âš ï¸ EMA è½å {delta_psnr:.2f} dBï¼Œè€ƒè™‘é™ä½ ema_rate")
    
    # Test 3: MSE Loss vs Reconstruction Loss
    recon_loss = F.mse_loss(x_ema_sampled, hres_z)
    if recon_loss > 0.01 and train/loss < 0.20:
        print("ğŸ”´ ç›®æ ‡å‡½æ•°é”™ä½ï¼šMSE ä½ä½†é‡å»ºå·®ã€‚æ˜¯ t åˆ†å¸ƒé—®é¢˜")
```

---

## ğŸ¯ æœ€ç»ˆä¼˜å…ˆçº§æ’åº

### âœ… å·²å®æ–½
1. **æ”¹åŠ¨ 1ï¼šéå‡åŒ€ t é‡‡æ · (t^1.5)**  
   é¢„æœŸæ”¶ç›Šï¼š+0.8~1.2 dB PSNRï¼Œ+0.04~0.06 SSIM  
   é£é™©ï¼šä½ï¼ˆç†è®ºå®‰å…¨ï¼‰  
   æ–‡ä»¶ï¼š`fmboost/flow.py`, `fmboost/trainer.py`, `inference.py`

### ğŸ”„ æ¨èæ¥ä¸‹æ¥å®æ–½
2. **æ”¹åŠ¨ Fï¼šè‡ªé€‚åº” EMA è¡°å‡**  
   é¢„æœŸæ”¶ç›Šï¼š+0.5 dB PSNRï¼ˆé‡‡æ ·ç”¨æ›´æ–°çš„æ¨¡å‹ï¼‰  
   é£é™©ï¼šä½ï¼ˆåªæ”¹è¶…å‚ï¼‰  
   ä¼˜å…ˆçº§ï¼š**é«˜**ï¼ˆç«‹å³æ‰§è¡Œï¼‰

3. **æ”¹åŠ¨ Gï¼šCosine å­¦ä¹ ç‡æ”¹ num_cycles=2.0**  
   é¢„æœŸæ”¶ç›Šï¼š+0.3~0.5 dB PSNRï¼ˆé¿å…ä¸­æœŸåœæ»ï¼‰  
   é£é™©ï¼šä½ï¼ˆåªæ”¹è°ƒåº¦ï¼‰  
   ä¼˜å…ˆçº§ï¼š**é«˜**ï¼ˆä¸‹æ¬¡è®­ç»ƒæ—¶åº”ç”¨ï¼‰

### âš ï¸ å¯é€‰ä½†æ¨èè¯Šæ–­
4. **æ”¹åŠ¨ Cï¼šReconstruction Loss ç›‘æ§**  
   é¢„æœŸæ”¶ç›Šï¼šè¯Šæ–­ä»·å€¼ï¼ˆæ— ç›´æ¥æŒ‡æ ‡æ”¶ç›Šï¼‰  
   é£é™©ï¼šä½  
   ä¼˜å…ˆçº§ï¼šä¸­ï¼ˆå…ˆçœ‹æ”¹åŠ¨ 1-3 çš„æ•ˆæœï¼‰

5. **æ”¹åŠ¨ Eï¼šæ¡ä»¶ Dropout**  
   é¢„æœŸæ”¶ç›Šï¼š+0.3~0.5 dBï¼ˆå¦‚æœå½“å‰è¿‡çº¦æŸï¼‰  
   é£é™©ï¼šä¸­ï¼ˆéœ€è¦å®éªŒéªŒè¯ï¼‰  
   ä¼˜å…ˆçº§ï¼šä½ï¼ˆå…ˆçœ‹æ”¹åŠ¨ 1-3 çš„æ•ˆæœï¼‰

### âŒ ä¸æ¨è
- ç›´æ¥åŠ  Latent L1 Lossï¼ˆç ´åæµåœºï¼‰
- Cross-Attentionï¼ˆè®¡ç®—æˆæœ¬ï¼Œgain ä¸å¤§ï¼‰

---

## æœ€ç»ˆæ•ˆæœé¢„æµ‹

å‡è®¾åœ¨å½“å‰ checkpointï¼ˆtrain/psnr â‰ˆ 23.5, train/ssim â‰ˆ 0.71ï¼‰ä¸Šç»§ç»­è®­ç»ƒï¼Œåº”ç”¨æ”¹åŠ¨ 1-3ï¼š

```
Initial:     PSNR 23.5 dB,  SSIM 0.71
æ”¹åŠ¨ 1:       +1.0 dB        +0.05  â†’  24.5 dB,  0.76
æ”¹åŠ¨ F:       +0.5 dB        +0.02  â†’  25.0 dB,  0.78
æ”¹åŠ¨ G:       +0.3 dB        +0.01  â†’  25.3 dB,  0.79

ç»¼åˆé¢„æœŸï¼š       +1.8 dB        +0.08  â†’  25.3 dB,  0.79
```

---

## å®æ–½æ­¥éª¤

### Step 1: éªŒè¯æ”¹åŠ¨ 1 ç”Ÿæ•ˆï¼ˆå·²å®Œæˆï¼‰
- [x] ä¿®æ”¹ `fmboost/flow.py` training_losses æ–¹æ³•
- [x] ä¿®æ”¹ `fmboost/trainer.py` forward å’Œ training_step æ–¹æ³•
- [x] ä¿®æ”¹ `inference.py` é»˜è®¤å‚æ•°ï¼ˆnum_steps=70, method=rk4, cfg_scale=1.1ï¼‰

### Step 2: é‡æ–°è®­ç»ƒ
```bash
python train.py \
  --config configs/flow400_64-128/unet-base_psu.yaml \
  --name restoration_exp_exponential_t \
  --use_wandb
```

é¢„æœŸè®­ç»ƒ 10k æ­¥åå°±èƒ½çœ‹åˆ° PSNR / SSIM çš„æå‡ã€‚

### Step 3: å®æ–½æ”¹åŠ¨ F å’Œ Gï¼ˆå¯é€‰ä½†æ¨èï¼‰
- ä¿®æ”¹ config.yaml ä¸­çš„ lr_scheduler_cfg
- è€ƒè™‘å®æ–½è‡ªé€‚åº” EMAï¼ˆéœ€è¦ä¿®æ”¹ fmboost/ema.pyï¼‰

### Step 4: è¯Šæ–­å’Œå¾®è°ƒ
æ ¹æ®è¯Šæ–­çŸ©é˜µçš„ç»“æœï¼Œå†³å®šæ˜¯å¦éœ€è¦è¿›ä¸€æ­¥æ”¹åŠ¨ã€‚

---

## å‚è€ƒèµ„æº

- **åŸè®ºæ–‡**ï¼šSchusterbauer et al., "Boosting Latent Diffusion with Flow Matching", ECCV 2024
- **Flow Matching åŸºç¡€**ï¼šLipman et al., "Flow Matching for Generative Modeling"
- **Rectified Flow**ï¼šLiu et al., "Rectified Flow: Generating Diverse High-Quality Images with Deterministic Generative Flow"

---

## å¸¸è§é—®é¢˜

### Q1: æ”¹åŠ¨ 1 ä¼šå¯¼è‡´è®­ç»ƒé€Ÿåº¦å˜æ…¢å—ï¼Ÿ
**A**: ä¸ä¼šã€‚åªæ˜¯æ”¹å˜äº†æ—¶é—´é‡‡æ ·åˆ†å¸ƒï¼Œè®¡ç®—é‡ç›¸åŒã€‚å¯èƒ½æ¢¯åº¦æ›´å¤§ï¼Œä½†è¿™æ˜¯å¥½äº‹ï¼ˆè®­ç»ƒåŠ¨åŠ›å¼ºï¼‰ã€‚

### Q2: å¯ä»¥åŒæ—¶åº”ç”¨æ”¹åŠ¨ 1ã€Fã€G å—ï¼Ÿ
**A**: å¯ä»¥ï¼Œä½†å»ºè®®åˆ†æ­¥ï¼šå…ˆç”¨æ”¹åŠ¨ 1 è®­ç»ƒ 20k æ­¥çœ‹æ•ˆæœï¼Œå†åŠ  F å’Œ Gã€‚è¿™æ ·ä¾¿äºè¯Šæ–­ã€‚

### Q3: noising_step=300 å¯ä»¥æ”¹å—ï¼Ÿ
**A**: å¯ä»¥ï¼Œä½†è¿™æ˜¯ä¸€ä¸ªä»»åŠ¡çº§å‚æ•°ï¼Œä¸å»ºè®®è½»æ˜“æ”¹åŠ¨ã€‚å¦‚æœæ”¹ï¼Œä¼šæ”¹å˜æ•´ä¸ªæµåœºçš„èµ·ç‚¹åˆ†å¸ƒï¼Œéœ€è¦é‡æ–°ä»å¤´è®­ç»ƒã€‚

### Q4: å¦‚ä½•å¿«é€ŸéªŒè¯æ”¹åŠ¨ 1 çš„æœ‰æ•ˆæ€§ï¼Ÿ
**A**: 
```bash
# ç”¨å°æ•°æ®é›†/å°‘æ­¥æ•°å¿«é€ŸéªŒè¯
python train.py --config ... --max_steps 2000 --limit_val_batches 2
# çœ‹ val/psnr å’Œ val/ssim çš„æå‡å¹…åº¦æ˜¯å¦ç¬¦åˆé¢„æœŸ
```

### Q5: å¦‚æœæ”¹åŠ¨ 1 å PSNR åè€Œä¸‹é™æ€ä¹ˆåŠï¼Ÿ
**A**: å¯èƒ½åŸå› ï¼š
1. æ–° checkpoint è¿˜æ²¡æ”¶æ•›ï¼ˆå¤šè®­ç»ƒå‡ ä¸ª epochï¼‰
2. é«˜å™ªé‡‡æ ·å¯¼è‡´çš„åˆæœŸæ¢¯åº¦ä¸ç¨³å®šï¼ˆå°è¯•é™ä½ LR 20%ï¼‰
3. UNet æ¶æ„ä¸é€‚åº”ï¼ˆrareï¼Œä½†å¯ä»¥è€ƒè™‘å›é€€åˆ° uniformï¼‰

---

## æ›´æ–°æ—¥å¿—

- **2026-01-03**ï¼šåˆå§‹ç‰ˆæœ¬ï¼ŒåŒ…å«æ”¹åŠ¨ 1 çš„å®æ–½å’Œæ”¹åŠ¨ Fã€G çš„ç†è®ºåˆ†æ
